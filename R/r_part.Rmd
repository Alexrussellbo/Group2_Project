---
title: "R"
author: 'Bo Liu (alexliu@umich.edu)'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Description of R-packages

  + [psych](https://cran.r-project.org/web/packages/psych/index.html) : A package for personality, psychometric, and psychological research including ***fa***, ***fa.diagram***, ***fa.parallel*** functions for factor analysis.
  + [tidyverse](https://www.rdocumentation.org/packages/tidyverse/versions/1.2.1): A set of packages for data processing.


```{r data_import, echo = TRUE, eval = TRUE, message=FALSE}
library(psych)
library(tidyverse)
```


## Conducting Factor Analysis {.tabset}

### Adequacy 
There are some methods to check the adequacy of factor analysis. In this project, we adopt two of them as follow.

####1. Kaiser-Meyer-Olkin (KMO) 
[Kaiser-Meyer-Olkin](https://www.statisticshowto.datasciencecentral.com/kaiser-meyer-olkin/) (KMO) is the measure of sampling adequacy, which varies between 0 and 1. The values closed to 1.0 usually means that it is useful to process the data used by factor analysis. If the value is less than 0.50, the results of the factor analysis probably won't be very useful.
```{r test1_KMO, echo = TRUE, eval = TRUE, message=FALSE}
mydata = bfi[1:25][complete.cases(bfi[1:25]),]
KMO(mydata)
```
Based on the results of KMO above, we observe the MSA for each item is greater than 0.74 and the overall MSA is 0.85, which illustrates it is useful if we use factor analysis on the data.


####2. Bartlett's Test of Sphericity
[Bartlett's Test of Sphericity](http://shodhganga.inflibnet.ac.in/bitstream/10603/9094/10/10_chapter%208.pdf) is the test for null hypothesis that the correlation matrix has an identity matrix. The significant p-value indicates null hypothesis that all off-diagonal correlations are zero is falsified.
```{r test2_bartlett, echo = TRUE, eval = TRUE, message=FALSE}
cortest.bartlett(mydata)
```
As we can see, the p-value of this test is significant, which means some of off-diagonal correlations are nonzero. It further suggests it is necessary to go ahead with factor analysis.


### Number of Factors
There are several ways to determine the number of factors. Now, we use two ways below.
####1. Screeplot
In [screeplot](https://www.researchgate.net/publication/276934102_The_Scree_Test_and_the_Number_of_Factors_a_Dynamic_Graphics_Approach), eigenvalues of correlation matrix represent variances explained by each factor (sometimes sums of squared factor loadings are used instead). The adequate number of factors is before the sudden downward inflextion of the plot. Eigenvalues sum to the number of items, so an eigenvalue more than 1 is more informative than a single average item.
```{r scree_plot, echo = TRUE, eval = TRUE, message=FALSE}
scree(mydata, fa = TRUE, pc = FALSE, main = "scree plot")
```

This scree plot shows that the eigenvalues for the first three factors are all strictly greater than 1, while the fourth eigenvalue is 0.97 which is almost equal to 1. So the first four factors account for most of the total variability in data (given by the eigenvalues). The remaining factors account for a very small proportion of the variability and are likely unimportant

####2. Parallel analysis
[Parallel analysis](https://opensiuc.lib.siu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1004&context=pb_pubs) is based on the comparison of eigenvalues of the actual data to those of the simulative data. The focal point is how many of the factors obtained from the actual data have an eigenvalue greater than that of the simulative data and accordingly the number of factors is decided. The number of factors at the point where the eigenvalue in the simulative data is greater than that of the actual data is considered significant.

```{r paralell, echo = TRUE, eval = TRUE, message=FALSE}
fa.parallel(mydata, fa = 'fa', error.bars = TRUE)
```

From Parallel Analysis scree plot, it is obvious that the six factors construct decided as a result of the examination of the eigenvalues is supported. Because the first six factors of the actual data have higher eigenvalues than the first six factors of the simulative data. 

Combined results with two methods mentioned above, we should compare 4, 5 and 6 factors cases and find out which case is the most intepretable.


### Factor Analysis
The function **fa.diagram** only shows the factor loadings greater than 0.3. It is easy to determine which one is the most interpretable. Let's get started with 4 factors by *varimax* rotation method. 
```{r factor4, echo = TRUE, eval = TRUE, message=FALSE}
fa4 = fa(mydata, nfactors = 4, fm="pa", max.iter = 100, rotate = "varimax")
fa.diagram(fa4, main = "# factors = 4", cut = .3)
```

As we can see, A1 and O4 do not have more dependence on each of these four factors than other items. It motivated us to try more factors.


```{r factor6, echo = TRUE, eval = TRUE, message=FALSE}
fa6 = fa(mydata, nfactors = 6, fm="pa", max.iter = 100, rotate = "varimax")
fa.diagram(fa6, main = "# factors = 6", cut = .3)
```

Here is the factor analysis with 6 factors. No items have strong relationship with the sixth factor. Therefore, we finally try the anaylsis with 5 factors.


```{r factor5, echo = TRUE, eval = TRUE, message=FALSE}
fa5 = fa(mydata, nfactors = 5, fm="pa", max.iter = 100, rotate = "varimax")
fa.diagram(fa5, main = "# factors = 5", cut = .3)

```

With 5 factors, it is easy to see each factors corresponse to five different items, which indicates it is more interpretable.


### Conclusions

####Factor Loadings
Below are the factor loadings with 5 factors cases. In this output, we can also see the factor loadings less than 0.3.
```{r loadings, echo = TRUE, eval = TRUE, message=FALSE}
print(fa5$loadings, cutoff=0, digits=3)
```


####Communalities
The communalities for the ith variable are computed by taking the sum of the squared loadings for that variable.
$$h_i^2 = \sum \limits_{j = 1}^m \hat{l}_{ij}^2$$
Communatities indicate the amount of variance in each variable that is accounted for. 

```{r communality, echo = TRUE, eval = TRUE, message=FALSE}
print(fa5$communality, cutoff=0, digits=3)
```
Generally, communatities between 0.0-0.4 should be considered as low ones. However, almost half of the communatities below are less than 0.4. So, we should check whether the communatities go much larger as the number of factors increasing.

```{r com_compare, echo = TRUE, eval = TRUE, message=FALSE}
com_matrix = c()
for(i in 1:11){
  result <- fa(mydata, nfactors = i, fm="pa", max.iter = 100, rotate = "varimax")
  com_matrix = rbind(com_matrix, result$communality)
}

as.tibble(com_matrix) %>%  
  gather(items, loadings) %>%
  group_by(items) %>%
  mutate(n = 1:11) %>%
  ggplot(aes(x = n ,y = loadings)) +
  geom_line(aes(colour = factor(items)), size = 0.5) +
  geom_vline(xintercept = 5, linetype="dotted", color = "blue", size=1) +
  geom_vline(xintercept = 6, linetype="dotted", color = "red", size=1) +
  xlab("Number of Factors") + ylab("Communalities")
```

As we can see, when the number of factors is larger than 5 or 6, there are no obvious increases in communatities. It further illustrates 5 factors are necessary for this analysis.

####Percentage of Variance Accounted For
We can use the eigenvalues to calculate the percentage of variance accounted for by each of the factors.
```{r var_2, echo = TRUE, eval = FALSE, message=FALSE}
nfactors = 5
fa5 = fa(mydata, nfactors = nfactors, fm="pa", max.iter = 100, rotate = "varimax")
var_per = 100*fa5$values[1:nfactors]/sum(fa5$values[1:nfactors])
cum_var = cumsum(var_per)
tb = as.data.frame(cbind(Var = var_per, Cum.var = cum_var))
tb %>%
  transmute(Factor = paste0('factor',1:nfactors), 
            Var = sprintf('%4.2f%%', Var)) 
```

The Factor Analysis has thus identified 5 putative factors that affect 25 self-report personality itemss. They can be categorized as follow:

| No. | Factors           | Main Items         | Proportion of explained variability |
|-----|-------------------|--------------------|-------------------------------------|
| 1   | Agreeableness     | A1, A2, A3, A4, A5 | 43.43%                              |
| 2   | Conscientiousness | C1, C2, C3, C4, C5 | 21.42%                              |
| 3   | Extraversion      | E1, E2, E3, E4, E5 | 14.62%                              |
| 4   | Neuroticism       | N1, N2, N3, N4, N5 | 11.50%                              |
| 5   | Openness          | O1, O2, O3, O4, O5 | 9.02%                               |










